{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/somaninggit/LearnBackend/blob/main/lab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IbHBPS9HRpYl"
      },
      "outputs": [],
      "source": [
        "// Bash File For Haddop\n",
        "\n",
        "export JAVA_HOME=$(readlink -f $(which javac) | awk 'BEGIN {FS=\"/bin\"} {print $1}')\n",
        "export PATH=$(echo $PATH):$(pwd)/bin\n",
        "export CLASSPATH=$(hadoop classpath)\n",
        "\n",
        "// Command to run Haddop\n",
        "\n",
        "javac -d . *.java\n",
        "echo Main-Class: oddeven.driver > Manifest.txt\n",
        "jar cfm oddeven.jar Manifest.txt oddeven/*.class\n",
        "echo 1 2 3 4 5 6 7 8 9 10 > oe.txt\n",
        "hadoop jar oddeven.jar oe.txt output\n",
        "cat output/*"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Q-1 Weather Report Data\n",
        "\n",
        "// driver.java\n",
        "\n",
        "package weather;\n",
        "import java.util.*;\n",
        "import java.io.*;\n",
        "import org.apache.hadoop.mapred.*;\n",
        "import org.apache.hadoop.io.*;\n",
        "import org.apache.hadoop.fs.Path;\n",
        "\n",
        "public class driver\n",
        "{\n",
        "    public static void main(String args[]) throws IOException\n",
        "    {\n",
        "   \t JobConf conf=new JobConf(driver.class);\n",
        "   \t conf.setMapperClass(mapper.class);\n",
        "   \t conf.setReducerClass(reducer.class);\n",
        "   \t conf.setOutputKeyClass(Text.class);\n",
        "   \t conf.setOutputValueClass(DoubleWritable.class);\n",
        "   \t FileInputFormat.addInputPath(conf, new Path(args[0]));\n",
        "   \t FileOutputFormat.setOutputPath(conf,new Path(args[1]));\n",
        "   \t JobClient.runJob(conf);\n",
        "    }\n",
        "}\n",
        "\n",
        "//mapper.java\n",
        "\n",
        "package weather;\n",
        "import java.util.*;\n",
        "import java.io.*;\n",
        "import org.apache.hadoop.mapred.*;\n",
        "import org.apache.hadoop.io.*;\n",
        "\n",
        "public class mapper extends MapReduceBase implements Mapper<LongWritable, Text,Text,DoubleWritable>{\n",
        "    public void map(LongWritable key , Text value , OutputCollector<Text,DoubleWritable> output, Reporter r) throws IOException\n",
        "    {\n",
        "   \t String line=value.toString();\n",
        "   \t String year=line.substring(15,19);\n",
        "   \t Double temp=Double.parseDouble(line.substring(87,92));\n",
        "   \t output.collect(new Text(year), new DoubleWritable(temp));\n",
        "    }\n",
        "}\n",
        "\n",
        "//reducer.java\n",
        "\n",
        "package weather;\n",
        "import java.util.*;\n",
        "import java.io.*;\n",
        "import org.apache.hadoop.mapred.*;\n",
        "import org.apache.hadoop.io.*;\n",
        "class reducer extends MapReduceBase implements Reducer<Text,DoubleWritable,Text,DoubleWritable> {\n",
        "    public void reduce(Text key, Iterator<DoubleWritable> value, OutputCollector<Text,DoubleWritable> output, Reporter r) throws IOException{\n",
        "   \t Double max=-9999.0;\n",
        "   \t Double min=9999.0;\n",
        "   \t while(value.hasNext()){\n",
        "   \t\t Double temp=value.next().get();\n",
        "   \t\t max=Math.max(max,temp);\n",
        "   \t\t min=Math.min(min,temp);\n",
        "   \t }\n",
        "   \t output.collect(new Text(\"Max temp at \"+ key), new DoubleWritable(max));\n",
        "   \t output.collect(new Text(\"Min temp at \"+ key), new DoubleWritable(min));\n",
        "    }\n",
        "}\n"
      ],
      "metadata": {
        "id": "EwKdxIhOSuVN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "//Q-2 EarthQuake Data\n",
        "\n",
        "//mapper.java\n",
        "\n",
        "package earthquake;\n",
        "import java.util.*;\n",
        "import java.io.*;\n",
        "import org.apache.hadoop.mapred.*;\n",
        "import org.apache.hadoop.io.*;\n",
        "public class mapper extends MapReduceBase implements Mapper<LongWritable, Text,Text,DoubleWritable>\n",
        "{\n",
        "    public void map(LongWritable key , Text value , OutputCollector<Text,DoubleWritable> output, Reporter r) throws IOException\n",
        "    {\n",
        "   \t String[] line=value.toString().split(\",\");\n",
        "   \t Double longi=Double.parseDouble(line[7]);\n",
        "   \t output.collect(new Text(line[11]), new DoubleWritable(longi));\n",
        "    }\n",
        "}\n",
        "\n",
        "//reducer.java\n",
        "\n",
        "package earthquake;\n",
        "import java.util.*;\n",
        "import java.io.*;\n",
        "import org.apache.hadoop.mapred.*;\n",
        "import org.apache.hadoop.io.*;\n",
        "class reducer extends MapReduceBase implements Reducer<Text,DoubleWritable,Text,DoubleWritable> {\n",
        "\n",
        "    public void reduce(Text key, Iterator<DoubleWritable> value, OutputCollector<Text,DoubleWritable> output, Reporter r) throws IOException\n",
        "    {\n",
        "   \t Double max=-9999.0;\n",
        "   \t while(value.hasNext())\n",
        "   \t {\n",
        "   \t\t Double temp=value.next().get();\n",
        "   \t\t max=Math.max(max,temp);\n",
        "   \t }\n",
        "   \t output.collect(new Text(key), new DoubleWritable(max));\n",
        "    }\n",
        "\n",
        "}\n",
        "\n"
      ],
      "metadata": {
        "id": "OVPG7lQVTlwD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "//Q-3 ODD Even\n",
        "\n",
        "//mapper.java\n",
        "\n",
        "package oddeven;\n",
        "import java.io.*;\n",
        "import java.util.*;\n",
        "import org.apache.hadoop.mapred.*;\n",
        "import org.apache.hadoop.io.*;\n",
        "\n",
        "public class mapper extends MapReduceBase implements Mapper<LongWritable , Text , Text , IntWritable>\n",
        "{\n",
        "    public void map(LongWritable key,Text value,OutputCollector<Text,IntWritable> output,Reporter r) throws IOException\n",
        "    {\n",
        "   \t String[] line=value.toString().split(\" \");\n",
        "   \t for(String num:line){\n",
        "   \t\t int number=Integer.parseInt(num);\n",
        "   \t\t if(number%2==0) {\n",
        "   \t\t\t output.collect(new Text(\"even\"),new IntWritable(number));\n",
        "   \t\t }\n",
        "   \t\t else{\n",
        "   \t\t\t output.collect(new Text(\"odd\"),new IntWritable(number));\n",
        "   \t\t }\n",
        "   \t }\n",
        "    }\n",
        "}\n",
        "\n",
        "//reducer.java\n",
        "\n",
        "package oddeven;\n",
        "import java.io.*;\n",
        "import java.util.*;\n",
        "import org.apache.hadoop.mapred.*;\n",
        "import org.apache.hadoop.io.*;\n",
        "public class reducer extends MapReduceBase implements Reducer<Text,IntWritable,Text,IntWritable>\n",
        "{\n",
        "    public void reduce(Text key,Iterator<IntWritable> value,OutputCollector<Text,IntWritable> output ,Reporter r) throws IOException\n",
        "    {\n",
        "   \t int sum=0,count=0;\n",
        "   \t while(value.hasNext()){\n",
        "   \t\t sum+=value.next().get();\n",
        "   \t\t count++;\n",
        "   \t }\n",
        "   \t output.collect(new Text(\"Sum of \"+key+\" Numbers\"),new IntWritable(sum));\n",
        "   \t output.collect(new Text(key+\" Number count\"),new IntWritable(count));\n",
        "    }\n",
        "}\n"
      ],
      "metadata": {
        "id": "9F-CyNqqUHwj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "//Q-4 Insurance Data\n",
        "\n",
        "//mapper.java\n",
        "\n",
        "package insurance;\n",
        "import java.io.*;\n",
        "import java.util.*;\n",
        "import org.apache.hadoop.mapred.*;\n",
        "import org.apache.hadoop.io.*;\n",
        "\n",
        "public class mapper extends MapReduceBase implements Mapper<LongWritable , Text , Text , IntWritable>\n",
        "{\n",
        "    public void map(LongWritable key,Text value,OutputCollector<Text,IntWritable> output,Reporter r) throws IOException\n",
        "    {\n",
        "   \t String[] line=value.toString().split(\",\");\n",
        "   \t output.collect(new Text(line[2]),new IntWritable(1));\n",
        "    }\n",
        "}\n",
        "\n",
        "//reducer.java\n",
        "\n",
        "package insurance;\n",
        "import java.io.*;\n",
        "import java.util.*;\n",
        "import org.apache.hadoop.mapred.*;\n",
        "import org.apache.hadoop.io.*;\n",
        "\n",
        "public class reducer extends MapReduceBase implements Reducer<Text,IntWritable,Text,IntWritable>\n",
        "{\n",
        "    public void reduce(Text key,Iterator<IntWritable> value,OutputCollector<Text,IntWritable> output ,Reporter r) throws IOException\n",
        "    {\n",
        "   \t int sum=0;\n",
        "   \t while(value.hasNext())\n",
        "   \t {\n",
        "   \t\t sum+=value.next().get();\n",
        "   \t }\n",
        "   \t output.collect(key,new IntWritable(sum));\n",
        "    }\n",
        "}\n"
      ],
      "metadata": {
        "id": "QIlUT5j7UObR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "//Q-5 Male Female DATA\n",
        "\n",
        "//mapper.java\n",
        "\n",
        "package employee;\n",
        "import java.io.*;\n",
        "import java.util.*;\n",
        "import org.apache.hadoop.mapred.*;\n",
        "import org.apache.hadoop.io.*;\n",
        "class mapper extends MapReduceBase implements Mapper<LongWritable , Text , Text , DoubleWritable> {\n",
        "\n",
        "    public void map(LongWritable key, Text value, OutputCollector<Text,DoubleWritable> output ,Reporter r) throws IOException\n",
        "    {\n",
        "   \t String[] line=value.toString().split(\"\\\\t\");\n",
        "   \t \t salary=Double.parseDouble(line[8]);\n",
        "   \t output.collect(new Text(line[3]), new DoubleWritable(salary));\n",
        "    }\n",
        "\n",
        "}\n",
        "\n",
        "//reducer.java\n",
        "\n",
        "package employee;\n",
        "import java.io.*;\n",
        "import java.util.*;\n",
        "import org.apache.hadoop.mapred.*;\n",
        "import org.apache.hadoop.io.*;\n",
        "class reducer extends MapReduceBase implements Reducer<Text,DoubleWritable,Text,DoubleWritable> {\n",
        "public void reduce(Text key,Iterator<DoubleWritable> value , OutputCollector<Text,DoubleWritable> output ,Reporter r) throws IOException\n",
        "    {\n",
        "   \t int count=0;\n",
        "   \t Double sum=0.0;\n",
        "   \t while(value.hasNext()){\n",
        "   \t\t sum+=value.next().get();\n",
        "   \t\t count+=1;\n",
        "   \t }\n",
        "   \t output.collect(new Text(key+\" Average\"), new DoubleWritable(sum/count));\n",
        "   \t output.collect(new Text(key+\" Count\"), new DoubleWritable(count));\n",
        "    }\n",
        "}\n"
      ],
      "metadata": {
        "id": "XfreSlfOUcgv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Q-6 Sales Record\n",
        "\n",
        "//mapper.java\n",
        "\n",
        "package sales;\n",
        "import java.io.*;\n",
        "import java.util.*;\n",
        "import org.apache.hadoop.mapred.*;\n",
        "import org.apache.hadoop.io.*;\n",
        "\n",
        "public class mapper extends MapReduceBase implements Mapper<LongWritable , Text , Text , IntWritable>\n",
        "{\n",
        "    public void map(LongWritable key,Text value,OutputCollector<Text,IntWritable> output,Reporter r) throws IOException\n",
        "    {\n",
        "   \t String[] line=value.toString().split(\",\");\n",
        "   \t int price=Integer.parseInt(line[2]);\n",
        "   \t String cardtype=line[3];\n",
        "   \t String Country=line[7];\n",
        "   \t output.collect(new Text(\"Country \"+Country),new IntWritable(price));\n",
        "   \t output.collect(new Text(\"CardType \"+cardtype),new IntWritable(1));\n",
        "    }\n",
        "}\n",
        "\n",
        "//reducer.java\n",
        "\n",
        "package sales;\n",
        "\n",
        "import java.io.*;\n",
        "import java.util.*;\n",
        "import org.apache.hadoop.mapred.*;\n",
        "import org.apache.hadoop.io.*;\n",
        "\n",
        "public class reducer extends MapReduceBase implements Reducer<Text,IntWritable,Text,IntWritable>\n",
        "{\n",
        "    public void reduce(Text key,Iterator<IntWritable> value,OutputCollector<Text,IntWritable> output ,Reporter r) throws IOException\n",
        "    {\n",
        "   \t int sum=0;\n",
        "   \t while(value.hasNext())\n",
        "   \t {\n",
        "   \t\t sum+=value.next().get();\n",
        "   \t }\n",
        "   \t output.collect(new Text(key),new IntWritable(sum));\n",
        "    }\n",
        "}\n"
      ],
      "metadata": {
        "id": "iCc26ZP5YqYl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "// Spark\n",
        "\n",
        "// Bash\n",
        "export JAVA_HOME=$(readlink -f $(which javac) | awk 'BEGIN {FS=\"/bin\"} {print $1}')\n",
        "if ! command -v spark-shell --version &> /dev/null\n",
        "then\n",
        "    export PATH=$(echo $PATH):$(pwd)/bin\n",
        "fi\n",
        "\n",
        "//command to execute\n",
        "spark-submit<python_filename>.py <inputFile> <outputfolder>"
      ],
      "metadata": {
        "id": "kNSb-fATZTVT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "//Q-1 Weather Report\n",
        "\n",
        "import sys\n",
        "if(len(sys.argv)!=3):\n",
        "\tprint(\"Provide Input File and Output Directory\")\n",
        "\tsys.exit(0)\n",
        "from pyspark import SparkContext\n",
        "sc =SparkContext()\n",
        "f = sc.textFile(sys.argv[1])\n",
        "temp=f.map(lambda x: (int(x[15:19]),int(x[87:92])))\n",
        "maxi=temp.reduceByKey(lambda a,b:a if a>b else b)\n",
        "maxi.saveAsTextFile(sys.argv[2])\n",
        "\n",
        "//minimum\n",
        "\n",
        "import sys\n",
        "if(len(sys.argv)!=3):\n",
        "\tprint(\"Provide Input File and Output Directory\")\n",
        "\tsys.exit(0)\n",
        "from pyspark import SparkContext\n",
        "sc =SparkContext()\n",
        "f = sc.textFile(sys.argv[1])\n",
        "temp=f.map(lambda x: (int(x[15:19]),int(x[87:92])))\n",
        "mini=temp.reduceByKey(lambda a,b:a if a<b else b)\n",
        "mini.saveAsTextFile(sys.argv[2])\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "V0PXSbGFZwWl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "//Q-2 EarthQuake Data\n",
        "\n",
        "//region and magnitude\n",
        "import sys\n",
        "if(len(sys.argv)!=3):\n",
        "\tprint(\"Provide Input File and Output Directory\")\n",
        "\tsys.exit(0)\n",
        "from pyspark import SparkContext\n",
        "sc =SparkContext()\n",
        "f = sc.textFile(sys.argv[1])\n",
        "temp=f.map(lambda x: (x.split(',')[11],float(x.split(',')[8])))\n",
        "maxi=temp.reduceByKey(lambda a,b:a if a>b else b)\n",
        "maxi.saveAsTextFile(sys.argv[2])\n",
        "\n",
        "//region and depth\n",
        "\n",
        "import sys\n",
        "if(len(sys.argv)!=3):\n",
        "\tprint(\"Provide Input File and Output Directory\")\n",
        "\tsys.exit(0)\n",
        "from pyspark import SparkContext\n",
        "sc =SparkContext()\n",
        "f = sc.textFile(sys.argv[1])\n",
        "temp=f.map(lambda x: (x.split(',')[11],float(x.split(',')[9])))\n",
        "maxi=temp.reduceByKey(lambda a,b:a if a>b else b)\n",
        "maxi.saveAsTextFile(sys.argv[2])\n",
        "\n",
        "//region and latitude\n",
        "\n",
        "import sys\n",
        "if(len(sys.argv)!=3):\n",
        "\tprint(\"Provide Input File and Output Directory\")\n",
        "\tsys.exit(0)\n",
        "from pyspark import SparkContext\n",
        "sc =SparkContext()\n",
        "f = sc.textFile(sys.argv[1])\n",
        "temp=f.map(lambda x: (x.split(',')[11],float(x.split(',')[6])))\n",
        "maxi=temp.reduceByKey(lambda a,b:a if a>b else b)\n",
        "maxi.saveAsTextFile(sys.argv[2])\n",
        "\n",
        "//region and longitude\n",
        "\n",
        "import sys\n",
        "if(len(sys.argv)!=3):\n",
        "\tprint(\"Provide Input File and Output Directory\")\n",
        "\tsys.exit(0)\n",
        "from pyspark import SparkContext\n",
        "sc =SparkContext()\n",
        "f = sc.textFile(sys.argv[1])\n",
        "temp=f.map(lambda x: (x.split(',')[11],float(x.split(',')[7])))\n",
        "maxi=temp.reduceByKey(lambda a,b:a if a>b else b)\n",
        "maxi.saveAsTextFile(sys.argv[2])\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "NE90Fxp-Z0S2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "//Q-3 Insurance Data\n",
        "\n",
        "//building name and count\n",
        "\n",
        "import sys\n",
        "if(len(sys.argv)!=3):\n",
        "\tprint(\"Provide Input File and Output Directory\")\n",
        "\tsys.exit(0)\n",
        "from pyspark import SparkContext\n",
        "sc =SparkContext()\n",
        "f = sc.textFile(sys.argv[1])\n",
        "temp=f.map(lambda x: (x.split(',')[16],1))\n",
        "data=temp.countByKey()\n",
        "dd=sc.parallelize(data.items())\n",
        "dd.saveAsTextFile(sys.argv[2])\n",
        "\n",
        "//country name and its frequency\n",
        "\n",
        "import sys\n",
        "if(len(sys.argv)!=3):\n",
        "\tprint(\"Provide Input File and Output Directory\")\n",
        "\tsys.exit(0)\n",
        "from pyspark import SparkContext\n",
        "sc =SparkContext()\n",
        "f = sc.textFile(sys.argv[1])\n",
        "temp=f.map(lambda x: (x.split(',')[2],1))\n",
        "data=temp.countByKey()\n",
        "dd=sc.parallelize(data.items())\n",
        "dd.saveAsTextFile(sys.argv[2])\n",
        "\n"
      ],
      "metadata": {
        "id": "nIr6RO6gaT87"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "//Q-4 Sales\n",
        "\n",
        "//country total sale and total number of product\n",
        "\n",
        "import sys\n",
        "if(len(sys.argv)!=3):\n",
        "\tprint(\"Provide Input File and Output Directory\")\n",
        "\tsys.exit(0)\n",
        "from pyspark import SparkContext\n",
        "sc =SparkContext()\n",
        "f = sc.textFile(sys.argv[1])\n",
        "temp=f.map(lambda x: (x.split(',')[7],1))\n",
        "data=temp.countByKey()\n",
        "dd=sc.parallelize(data.items())\n",
        "dd.saveAsTextFile(sys.argv[2])\n",
        "\n",
        "//total sale and payment frequency\n",
        "\n",
        "import sys\n",
        "if(len(sys.argv)!=3):\n",
        "\tprint(\"Provide Input File and Output Directory\")\n",
        "\tsys.exit(0)\n",
        "from pyspark import SparkContext\n",
        "sc =SparkContext()\n",
        "f = sc.textFile(sys.argv[1])\n",
        "temp=f.map(lambda x: (x.split(',')[3],1))\n",
        "data=temp.countByKey()\n",
        "dd=sc.parallelize(data.items())\n",
        "dd.saveAsTextFile(sys.argv[2])\n"
      ],
      "metadata": {
        "id": "nSgqvmbCapxV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "//PIG\n",
        "\n",
        "//bash file\n",
        "\n",
        "export JAVA_HOME=$(readlink -f $(which javac) | awk 'BEGIN {FS=\"/bin\"} {print $1}')\n",
        "if ! command -v pig &> /dev/null\n",
        "then\n",
        "export PATH=$(echo $PATH):$(pwd)/bin\n",
        "fi\n",
        "\n",
        "//\n",
        "vi p1.pig\n",
        "pig p1.pig"
      ],
      "metadata": {
        "id": "Mk5117qJa5FK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "//Q-5 Student_details\n",
        "\n",
        "// Filter student\n",
        "\n",
        "student_details = LOAD '/home/msrit/Downloads/pig/test/Filter/student_details.txt' USING\n",
        "PigStorage(',') as (id:int, firstname:chararray, lastname:chararray, age:int, phone:chararray,\n",
        "city:chararray);\n",
        "filter_data = FILTER student_details BY city == 'Chennai';\n",
        "Dump filter_data;\n",
        "\n",
        "//group by age\n",
        "\n",
        "student = LOAD 'student_details.txt' USING PigStorage(',') as (id:int, firstname:chararray,\n",
        "lastname:chararray, age:int, phone:chararray, city:chararray);\n",
        "group_data = GROUP student by age;\n",
        "Dump group_data;"
      ],
      "metadata": {
        "id": "MhZraaj7blHT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "//Q-6 Customer Details\n",
        "\n",
        "// join\n",
        "\n",
        "customers = LOAD 'customer.txt' USING PigStorage(',') as (id:int, name:chararray, age:int,\n",
        "address:chararray, salary:int);\n",
        "orders = LOAD 'order.txt' USING PigStorage(',') as (oid:int, date:chararray, customer_id:int,\n",
        "amount:int);\n",
        "join_result = JOIN customers BY id, orders BY customer_id;\n",
        "Dump join_result\n",
        "\n",
        "//order by age\n",
        "\n",
        "student = LOAD 'student_details.txt' USING PigStorage(',') as (id:int, firstname:chararray,\n",
        "lastname:chararray, age:int, phone:chararray, city:chararray);\n",
        "student_order = ORDER student BY age DESC;\n",
        "student_limit = LIMIT student_order 4;\n",
        "Dump student_limit;"
      ],
      "metadata": {
        "id": "T7WTUW4_bp5q"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}